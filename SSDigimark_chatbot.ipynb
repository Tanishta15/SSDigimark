{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s_LukzK029k2"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers accelerate bitsandbytes pandas openpyxl --quiet\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwI1hZIYT7vq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YTiTp8l31Bo"
      },
      "outputs": [],
      "source": [
        "faq_df = pd.read_excel(\"/content/drive/MyDrive/faqs.xlsx\")\n",
        "faq_df.fillna(\"\", inplace=True)  # Prevent NaN errors\n",
        "faq_df['question'] = faq_df['question'].str.lower().str.strip()\n",
        "faq_df['keywords'] = faq_df['keywords'].str.lower().str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a-PeeFbg35In"
      },
      "outputs": [],
      "source": [
        "login(token=\"\")#Enter your token here \n",
        "\n",
        "# Prepare model\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load the model with 8-bit quantization and enable CPU offload for 32-bit modules\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    # Add this parameter to enable offloading 32-bit modules to CPU if necessary\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLLoii8g3-AV"
      },
      "outputs": [],
      "source": [
        "chatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=150)\n",
        "\n",
        "# System Prompt\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are SS DigiBot, an assistant for SS DigiMark, a digital marketing agency. \"\n",
        "    \"Answer user questions in a professional and helpful tone. \"\n",
        ")\n",
        "\n",
        "# FAQ lookup with keyword fallback\n",
        "def lookup_faq(user_input):\n",
        "    user_input_lower = user_input.lower().strip()\n",
        "\n",
        "    # 1. Exact match (in question)\n",
        "    for _, row in faq_df.iterrows():\n",
        "        if user_input_lower in row['question']:\n",
        "            return row['answer']\n",
        "\n",
        "    # 2. Keyword match\n",
        "    for _, row in faq_df.iterrows():\n",
        "        keywords = [kw.strip() for kw in row['keywords'].split(',')]\n",
        "        if any(kw in user_input_lower for kw in keywords):\n",
        "            return row['answer']\n",
        "\n",
        "    return None\n",
        "\n",
        "# Fallback to Mistral\n",
        "def get_mistral_response(user_input):\n",
        "    prompt = f\"<s>[INST] {SYSTEM_PROMPT} {user_input} [/INST]\"\n",
        "    output = chatbot(prompt)[0]['generated_text']\n",
        "    return output.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "# Main response function\n",
        "def respond(user_input):\n",
        "    answer = lookup_faq(user_input)\n",
        "    if answer:\n",
        "        return answer\n",
        "    return get_mistral_response(user_input)\n",
        "\n",
        "# Interface\n",
        "print(\"SS DigiBot: Hello! How can I help you? (Type 'exit' to quit)\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.strip().lower() in ['exit', 'quit', 'done']:\n",
        "        print(\"SS DigiBot: Goodbye!\")\n",
        "        break\n",
        "    print(\"SS DigiBot:\", respond(user_input))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
